{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Improving Computer Vision Accuracy using Convolutions",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukdame/IntroTensorflow/blob/master/Improving_Computer_Vision_Accuracy_using_Convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6gHiH-I7uFa",
        "colab_type": "text"
      },
      "source": [
        "#Improving Computer Vision Accuracy using Convolutions\n",
        "\n",
        "In the previous lessons you saw how to do fashion recognition using a Deep Neural Network (DNN) containing three layers -- the input layer (in the shape of the data), the output layer (in the shape of the desired output) and a hidden layer. You experimented with the impact of different sized of hidden layer, number of training epochs etc on the final accuracy.\n",
        "\n",
        "For convenience, here's the entire code again. Run it and take a note of the test accuracy that is printed out at the end. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcsRtq9OLorS",
        "colab_type": "code",
        "outputId": "25879c53-5bbb-4aed-bd23-a969d0fe4eb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# NORMALIZATION\n",
        "training_images=training_images / 255.0\n",
        "test_images=test_images / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "model.summary()\n",
        "print(\"Evalutaion on test set images\")\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4941 - acc: 0.8273\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 4s 74us/sample - loss: 0.3744 - acc: 0.8652\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 4s 75us/sample - loss: 0.3368 - acc: 0.8770\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 4s 74us/sample - loss: 0.3142 - acc: 0.8856\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 4s 75us/sample - loss: 0.2933 - acc: 0.8923\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_4 (Flatten)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              multiple                  100480    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              multiple                  1290      \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Evalutaion on test set images\n",
            "10000/10000 [==============================] - 1s 57us/sample - loss: 0.3781 - acc: 0.8653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zldEXSsF8Noz",
        "colab_type": "text"
      },
      "source": [
        "Your accuracy is probably about 89% on training and 87% on validation...not bad...But how do you make that even better? One way is to use something called Convolutions. I'm not going to details on Convolutions here, but the ultimate concept is that they narrow down the content of the image to focus on specific, distinct, details. \n",
        "\n",
        "If you've ever done image processing using a filter (like this: https://en.wikipedia.org/wiki/Kernel_(image_processing)) then convolutions will look very familiar.\n",
        "\n",
        "In short, you take an array (usually 3x3 or 5x5) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can do things like edge detection. So, for example, if you look at the above link, you'll see a 3x3 that is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has the edges enhanced.\n",
        "\n",
        "This is perfect for computer vision, because often it's features that can get highlighted like this that distinguish one item for another, and the amount of information needed is then much less...because you'll just train on ***the highlighted features.***\n",
        "\n",
        "That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focussed, and possibly more accurate.\n",
        "\n",
        "Run the below code -- this is the same neural network as earlier, but this time with Convolutional layers added first. It will take longer, but look at the impact on the <u>accuracy</u>:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0tFgT1MMKi6",
        "colab_type": "code",
        "outputId": "83ffd73d-eef5-4723-891f-c568ab592b25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "print(\"Evalutaion on test set images\")\n",
        "test_loss = model.evaluate(test_images, test_labels)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.4437 - acc: 0.8398\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.2969 - acc: 0.8913\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.2492 - acc: 0.9083\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 6s 108us/sample - loss: 0.2162 - acc: 0.9194\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.1886 - acc: 0.9291\n",
            "Evalutaion on test set images\n",
            "10000/10000 [==============================] - 1s 69us/sample - loss: 0.2568 - acc: 0.9091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRLfZ0jt-fQI",
        "colab_type": "text"
      },
      "source": [
        "It's likely gone up to about 93% on the training data and 91% on the validation data. \n",
        "\n",
        "That's significant, and a step in the right direction!\n",
        "\n",
        "Try running it for more epochs -- say about 20, and explore the results! But while the results might seem really good, the validation results may actually go down, due to something called 'overfitting' which will be discussed later. \n",
        "\n",
        "(In a nutshell, 'overfitting' occurs when the network learns the data from the training set really well, but it's too specialised to only that data, and as a result is less effective at seeing *other* data. For example, if all your life you only saw red shoes, then when you see a red shoe you would be very good at identifying it, but blue suade shoes might confuse you...and you know you should never mess with my blue suede shoes.)\n",
        "\n",
        "Then, look at the code again, and see, step by step how the Convolutions were built:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaLX5cgI_JDb",
        "colab_type": "text"
      },
      "source": [
        "Step 1 is to gather the data. You'll notice that there's a bit of a change here in that the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape. \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS_W_INc_kJQ",
        "colab_type": "text"
      },
      "source": [
        "Next is to define your model. Now instead of the input layer at the top, you're going to add a Convolution. The parameters are:\n",
        "\n",
        "1. The number of convolutions you want to generate. Purely arbitrary, but good to start with something in the order of 32\n",
        "2. The size of the Convolution, in this case a 3x3 grid\n",
        "3. The activation function to use -- in this case we'll use relu, which you might recall is the equivalent of returning x when x>0, else returning 0\n",
        "4. In the first layer, the shape of the input data.\n",
        "\n",
        "You'll follow the Convolution with a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convlution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. Without going into too much detail here, the idea is that it creates a 2x2 array of pixels, and picks the biggest one, thus turning 4 pixels into 1. It repeats this across the image, and in so doing halves the number of horizontal, and halves the number of vertical pixels, effectively reducing the image by 25%.\n",
        "\n",
        "You can call model.summary() to see the size and shape of the network, and you'll notice that after every MaxPooling layer, the image size is reduced in this way. \n",
        "\n",
        "\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMorM6daADjA",
        "colab_type": "text"
      },
      "source": [
        "Add another convolution\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b1-x-kZF4_tC"
      },
      "source": [
        "Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Flatten(),\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPtqR23uASjX",
        "colab_type": "text"
      },
      "source": [
        "The same 128 dense layers, and 10 output layers as in the pre-convolution example:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0GSsjUhAaSj",
        "colab_type": "text"
      },
      "source": [
        "Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the test set.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXx_LX3SAlFs",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing the Convolutions and Pooling\n",
        "\n",
        "This code will show us the convolutions graphically. The print (test_labels[;100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-6nX4QsOku6",
        "colab_type": "code",
        "outputId": "609b4639-a9d0-4ae9-dab3-4b2d488ae1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "print(test_labels[:100])\n",
        "print(type(test_labels), test_labels.size, test_labels.shape)\n",
        "print(test_labels)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
            " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
            " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n",
            "<class 'numpy.ndarray'> 10000 (10000,)\n",
            "[9 2 1 ... 8 1 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtRStXVBPwcT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "c23be9bf-128c-4e62-b70c-06f0fe6319c3"
      },
      "source": [
        "index_list = {}\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "for x in range(10):\n",
        "  index_list[x] = [] \n",
        "for inx, x in enumerate(list(test_labels[:100])):\n",
        "  index_list[x].append(inx) \n",
        "for x in range(10):\n",
        "  print(x, class_names[x], index_list[x])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 T-shirt/top [19, 27, 35, 59, 71, 85, 88, 96]\n",
            "1 Trouser [2, 3, 5, 15, 24, 41, 47, 64, 65, 76, 80, 94, 97]\n",
            "2 Pullover [1, 16, 20, 46, 48, 49, 54, 55, 66, 72, 74, 77, 87, 99]\n",
            "3 Dress [13, 29, 32, 33, 42, 67, 75, 86, 91]\n",
            "4 Coat [6, 10, 14, 17, 25, 50, 51, 57, 79, 98]\n",
            "5 Sandal [8, 11, 21, 37, 52, 63, 82, 84, 90]\n",
            "6 Shirt [4, 7, 26, 40, 44, 73, 89, 92]\n",
            "7 Sneaker [9, 12, 22, 36, 38, 43, 45, 60, 61, 70, 93]\n",
            "8 Bag [18, 30, 31, 34, 53, 56, 58, 62, 69, 78, 81, 95]\n",
            "9 Ankle boot [0, 23, 28, 39, 68, 83]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FGsHhv6JvDx",
        "colab_type": "code",
        "outputId": "684f2ecc-ec82-4023-d992-0bd2fa084b20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(1,4)\n",
        "FIRST_IMAGE=0\n",
        "CONVOLUTION_NUMBER = 1\n",
        "\n",
        "from tensorflow.keras import models\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "print(model.layers[0])\n",
        "print(layer_outputs[0])\n",
        "print(model.input[0])\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[x].grid(False)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3490124240>\n",
            "Tensor(\"conv2d_2/Relu:0\", shape=(?, 26, 26, 64), dtype=float32)\n",
            "Tensor(\"strided_slice_12:0\", shape=(28, 28, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABqCAYAAAClIwp2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADrqLVwla4Hc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "44e26c0f-29fb-44ae-ef71-4bf97015880b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(3,4)\n",
        "FIRST_IMAGE=0\n",
        "SECOND_IMAGE=7\n",
        "THIRD_IMAGE=26\n",
        "CONVOLUTION_NUMBER = 1\n",
        "\n",
        "from tensorflow.keras import models\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAD7CAYAAAC2a1UBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KVPZqgHo5Ux",
        "colab_type": "text"
      },
      "source": [
        "EXERCISES\n",
        "\n",
        "1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.\n",
        "\n",
        "2. Remove the final Convolution. What impact will this have on accuracy or training time?\n",
        "\n",
        "3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.\n",
        "\n",
        "4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it. \n",
        "\n",
        "5. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpYRidBXpBPM",
        "colab_type": "code",
        "outputId": "3e2c4029-7884-478a-f585-d13ee4eb8c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "CONVOLUTION_NUMBER = 16\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(CONVOLUTION_NUMBER, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Train on 60000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.1666 - acc: 0.9516\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0560 - acc: 0.9833\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0381 - acc: 0.9884\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 97us/sample - loss: 0.0256 - acc: 0.9920\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 97us/sample - loss: 0.0177 - acc: 0.9944\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0128 - acc: 0.9955\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0097 - acc: 0.9967\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0077 - acc: 0.9976\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 97us/sample - loss: 0.0057 - acc: 0.9983\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0052 - acc: 0.9984\n",
            "10000/10000 [==============================] - 1s 68us/sample - loss: 0.0554 - acc: 0.9857\n",
            "0.9857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQaD0v8bfESZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "b69d27ae-14d4-4f0f-881f-3d9db16da96d"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "CONVOLUTION_NUMBER = 32\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(CONVOLUTION_NUMBER, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Train on 60000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.1473 - acc: 0.9565\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0517 - acc: 0.9842\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 97us/sample - loss: 0.0321 - acc: 0.9902\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0224 - acc: 0.9926\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0150 - acc: 0.9950\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0113 - acc: 0.9962\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.0074 - acc: 0.9977\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0071 - acc: 0.9975\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 99us/sample - loss: 0.0059 - acc: 0.9981\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 99us/sample - loss: 0.0050 - acc: 0.9983\n",
            "10000/10000 [==============================] - 1s 70us/sample - loss: 0.0624 - acc: 0.9873\n",
            "0.9873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DQzQES_fGNF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "e3e48cb9-57ac-40d7-a0d4-a02d900c26f1"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "CONVOLUTION_NUMBER = 64\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(CONVOLUTION_NUMBER, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Train on 60000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.1351 - acc: 0.9604\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.0470 - acc: 0.9853\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.0295 - acc: 0.9905\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.0188 - acc: 0.9938\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.0117 - acc: 0.9960\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.0085 - acc: 0.9969\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.0067 - acc: 0.9980\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.0059 - acc: 0.9979\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.0048 - acc: 0.9983\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.0044 - acc: 0.9986\n",
            "10000/10000 [==============================] - 1s 72us/sample - loss: 0.0749 - acc: 0.9843\n",
            "0.9843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iJvPKjMfK-y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "6b71974c-8e2f-46d0-9f68-3a3bdebe1cee"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "CONVOLUTION_NUMBER = 128\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('acc')>0.9999):\n",
        "      print(\"\\nReached 99.99% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()   \n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(CONVOLUTION_NUMBER, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10, callbacks=[callbacks])\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Train on 60000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 7s 123us/sample - loss: 0.1241 - acc: 0.9625\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0437 - acc: 0.9864\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0248 - acc: 0.9924\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0167 - acc: 0.9942\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0103 - acc: 0.9966\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0091 - acc: 0.9969\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0065 - acc: 0.9980\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0045 - acc: 0.9986\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 7s 117us/sample - loss: 0.0055 - acc: 0.9980\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0046 - acc: 0.9985\n",
            "10000/10000 [==============================] - 1s 73us/sample - loss: 0.0498 - acc: 0.9880\n",
            "0.988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_id1ZFUf_LA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "b624d6a8-ca48-412d-89d3-b1bfad6d6a85"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "CONVOLUTION_NUMBER = 256\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('acc')>0.9999):\n",
        "      print(\"\\nReached 99.99% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()   \n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(CONVOLUTION_NUMBER, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10, callbacks=[callbacks])\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Train on 60000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 9s 144us/sample - loss: 0.1204 - acc: 0.9628\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0413 - acc: 0.9872\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 9s 142us/sample - loss: 0.0235 - acc: 0.9926\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 9s 142us/sample - loss: 0.0164 - acc: 0.9947\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0108 - acc: 0.9965\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0077 - acc: 0.9976\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0069 - acc: 0.9977\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0054 - acc: 0.9981\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0048 - acc: 0.9984\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0041 - acc: 0.9988\n",
            "10000/10000 [==============================] - 1s 85us/sample - loss: 0.0571 - acc: 0.9872\n",
            "0.9872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOjzkcX0gEbU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "307d43fa-34b5-4aec-d24c-0d31f76a6403"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "CONVOLUTION_NUMBER = 512\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('acc')>0.9999):\n",
        "      print(\"\\nReached 99.99% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()   \n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(CONVOLUTION_NUMBER, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10, callbacks=[callbacks])\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Train on 60000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 12s 201us/sample - loss: 0.1130 - acc: 0.9655\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 12s 198us/sample - loss: 0.0402 - acc: 0.9870\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 12s 200us/sample - loss: 0.0229 - acc: 0.9921\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 12s 198us/sample - loss: 0.0148 - acc: 0.9951\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 12s 198us/sample - loss: 0.0112 - acc: 0.9964\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 12s 198us/sample - loss: 0.0073 - acc: 0.9977\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 12s 198us/sample - loss: 0.0084 - acc: 0.9973\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 12s 199us/sample - loss: 0.0069 - acc: 0.9977\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 12s 199us/sample - loss: 0.0049 - acc: 0.9983\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 12s 197us/sample - loss: 0.0052 - acc: 0.9981\n",
            "10000/10000 [==============================] - 1s 103us/sample - loss: 0.0602 - acc: 0.9875\n",
            "0.9875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O--Wkkd_gFmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}